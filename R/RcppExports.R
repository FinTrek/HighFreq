# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Calculate the eigen decomposition of the covariance matrix of returns using
#' \emph{RcppArmadillo}.
#' 
#' @param mat_rix A numeric \emph{matrix} of returns data.
#'
#' @return A list with two elements: a numeric \emph{vector} of eigenvalues 
#'   (named "values"), and a numeric \emph{matrix} of eigenvectors (named
#'   "vectors").
#'
#' @details The function \code{calc_eigen()} first calculates the covariance 
#'   matrix of the \code{mat_rix}, and then calculates its eigen decomposition.
#'
#' @examples
#' \dontrun{
#' # Create random matrix
#' mat_rix <- matrix(rnorm(500), nc=5)
#' # Calculate eigen decomposition
#' ei_gen <- HighFreq::calc_eigen(scale(mat_rix, scale=FALSE))
#' # Calculate PCA
#' pc_a <- prcomp(mat_rix)
#' # Compare PCA with eigen decomposition
#' all.equal(pc_a$sdev^2, drop(ei_gen$values))
#' all.equal(abs(unname(pc_a$rotation)), abs(ei_gen$vectors))
#' }
#' @export
calc_eigen <- function(mat_rix) {
    .Call('_HighFreq_calc_eigen', PACKAGE = 'HighFreq', mat_rix)
}

#' Calculate the regularized inverse of the covariance matrix of returns using 
#' \emph{RcppArmadillo}.
#' 
#' @param mat_rix A numeric \emph{matrix} of returns data.
#' @param max_eigen An \emph{integer} equal to the regularization intensity
#'   (the number of eigenvalues and eigenvectors used for calculating the 
#'   regularized inverse).
#'
#' @return A numeric \emph{matrix} equal to the regularized inverse. 
#'
#' @details The function \code{calc_inv()} first calculates the covariance 
#'   matrix of the \code{mat_rix}, and then it calculates the regularized
#'   inverse from the truncated eigen decomposition.
#'   It uses only the largest \code{max_eigen} eigenvalues and their
#'   corresponding eigenvectors.
#'
#' @examples
#' \dontrun{
#' # Create random matrix
#' mat_rix <- matrix(rnorm(500), nc=5)
#' max_eigen <- 3
#' # Calculate regularized inverse using RcppArmadillo
#' in_verse <- HighFreq::calc_inv(mat_rix, max_eigen)
#' # Calculate regularized inverse from eigen decomposition in R
#' ei_gen <- eigen(cov(mat_rix))
#' inverse_r <-  ei_gen$vectors[, 1:max_eigen] %*% (t(ei_gen$vectors[, 1:max_eigen]) / ei_gen$values[1:max_eigen])
#' # Compare RcppArmadillo with R
#' all.equal(in_verse, inverse_r)
#' }
#' @export
calc_inv <- function(mat_rix, max_eigen) {
    .Call('_HighFreq_calc_inv', PACKAGE = 'HighFreq', mat_rix, max_eigen)
}

#' Scale (standardize) the columns of a \emph{matrix} of data using
#' \emph{RcppArmadillo}.
#' 
#' @param mat_rix A numeric \emph{matrix} of data.
#' @param use_median A \emph{Boolean} argument: if \code{TRUE} then the 
#'   centrality (central tendency) is calculated as the \emph{median} and the 
#'   dispersion is calculated as the \emph{median absolute deviation}
#'   (\emph{MAD}).
#'   If \code{use_median} is \code{FALSE} then the centrality is calculated as 
#'   the \emph{mean} and the dispersion is calculated as the \emph{standard
#'   deviation}. (The default is \code{FALSE})
#'
#' @return A numeric \emph{matrix} with the same dimensions as the input
#'   argument \code{mat_rix}.
#'
#' @details The function \code{calc_scaled()} scales (standardizes) the columns
#'   of the \code{mat_rix} argument using \emph{RcppArmadillo}.
#'   If the argument \code{use_median} is \code{FALSE} (the default), then it
#'   performs a similar calculation as the standard \emph{R} function
#'   \code{scale()}, and it calculates the centrality (central tendency) as the
#'   \emph{mean} and the dispersion as the \emph{standard deviation}.
#'   If the argument \code{use_median} is \code{TRUE}, then it calculates the
#'   centrality as the \emph{median} and the dispersion as the \emph{median
#'   absolute deviation} (\emph{MAD}).
#'   
#'   The function \code{calc_scaled()} uses \emph{RcppArmadillo} and is about
#'   \emph{5} times faster than function \code{scale()}, for a matrix with
#'   \emph{1,000} rows and \emph{20} columns.
#'   
#' @examples
#' \dontrun{
#' mat_rix <- matrix(rnorm(20000), nc=20)
#' scale_d <- calc_scaled(mat_rix=mat_rix, use_median=FALSE)
#' scale_d2 <- scale(mat_rix)
#' all.equal(scale_d, scale_d2, check.attributes=FALSE)
#' library(microbenchmark)
#' summary(microbenchmark(
#'   pure_r=scale(mat_rix),
#'   rcpp=calc_scaled(mat_rix=mat_rix, use_median=FALSE),
#'   times=100))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' @export
calc_scaled <- function(mat_rix, use_median = FALSE) {
    .Call('_HighFreq_calc_scaled', PACKAGE = 'HighFreq', mat_rix, use_median)
}

#' Calculate the variance of a vector using \emph{Rcpp}.
#' 
#' @param vec_tor A numeric \emph{vector} of data.
#'
#' @return A single \emph{numeric} value.
#'
#' @details The function \code{vari_ance()} calculates the variance of a vector
#'   using \emph{Rcpp}. The function \code{vari_ance()} is slightly faster than
#'   the \emph{R} function \code{var()}.
#' 
#' @examples
#' \dontrun{
#' # calculate variance 
#' HighFreq::vari_ance(rnorm(1000))
#' }
#' @export
vari_ance <- function(vec_tor) {
    .Call('_HighFreq_vari_ance', PACKAGE = 'HighFreq', vec_tor)
}

#' Perform multivariate linear regression using RcppArmadillo.
#' 
#' @param res_ponse A numeric \emph{vector} of response data.
#' @param de_sign A numeric \emph{matrix} of design (predictor i.e.
#'   explanatory) data.
#' 
#' @return A named list with three elements: a numeric \emph{matrix} of 
#'   coefficients (named \emph{"coefficients"}), the \emph{z-score} of the last
#'   residual (named \emph{"z_score"}), and a numeric \emph{vector} with the 
#'   R-squared and F-statistic (named \emph{"stats"}). The numeric 
#'   \emph{matrix} of coefficients named \emph{"coefficients"} containes the 
#'   alpha and beta coefficients, and their \emph{t-values} and 
#'   \emph{p-values}.
#'
#' @details The function \code{calc_lm()} performs the same calculations as the
#'   function \code{lm()} from package \emph{stats}.
#'   It uses \emph{RcppArmadillo} and is about \emph{10} times faster than
#'   \code{lm()}.
#'   The code was inspired by this article (but it's not identical to it):
#'   http://gallery.rcpp.org/articles/fast-linear-model-with-armadillo/
#'
#' @examples
#' \dontrun{
#' # Define design matrix with explanatory variables
#' len_gth <- 100; n_var <- 5
#' de_sign <- matrix(rnorm(n_var*len_gth), nc=n_var)
#' # response equals linear form plus error terms
#' weight_s <- rnorm(n_var)
#' res_ponse <- -3 + de_sign %*% weight_s + rnorm(len_gth, sd=0.5)
#' # perform multivariate regression using lm()
#' reg_model <- lm(res_ponse ~ de_sign)
#' sum_mary <- summary(reg_model)
#' # perform multivariate regression using calc_lm()
#' reg_model_arma <- calc_lm(res_ponse=res_ponse, de_sign=de_sign)
#' reg_model_arma$coefficients
#' # compare the outputs of both functions
#' all.equal(reg_model_arma$coefficients[, "coeff"], unname(coef(reg_model)))
#' all.equal(unname(reg_model_arma$coefficients), unname(sum_mary$coefficients))
#' all.equal(drop(reg_model_arma$residuals), unname(reg_model$residuals))
#' all.equal(unname(reg_model_arma$stats), c(sum_mary$r.squared, unname(sum_mary$fstatistic[1])))
#' }
#' @export
calc_lm <- function(res_ponse, de_sign) {
    .Call('_HighFreq_calc_lm', PACKAGE = 'HighFreq', res_ponse, de_sign)
}

#' Calculate the rolling sum over a vector using \emph{Rcpp}.
#' 
#' @param vec_tor A numeric \emph{vector} of data.
#' @param look_back The length of the look-back interval, equal to the number 
#'   of elements of data used for calculating the sum.
#'
#' @return A numeric \emph{vector} of the same length as the argument
#'   \code{vec_tor}.
#'
#' @details The function \code{roll_sum()} calculates a \emph{vector} of 
#'   rolling sums, over a \emph{vector} of data, using \emph{Rcpp}.  The
#'   function \code{roll_sum()} is over \emph{6} times faster than
#'   \code{rutils::roll_sum()} which uses vectorized \emph{R}.
#'
#' @examples
#' \dontrun{
#' # calculate rolling sums over 11-period intervals
#' sum_rolling <- HighFreq::roll_sum(rnorm(1000), look_back=11)
#' }
#' @export
roll_sum <- function(vec_tor, look_back) {
    .Call('_HighFreq_roll_sum', PACKAGE = 'HighFreq', vec_tor, look_back)
}

#' Calculate the rolling weighted sum over a vector of data using
#' \emph{RcppArmadillo}.
#' 
#' @param vec_tor A numeric \emph{vector} of data.
#' @param weight_s A numeric \emph{vector} of weights.
#'
#' @return A numeric \emph{vector} of the same length as the argument
#'   \code{vec_tor}.
#'
#' @details The function \code{roll_wsum()} calculates the rolling weighted sum
#'   of a vector over its past values (a convolution with the \emph{vector} of 
#'   weights), using \emph{RcppArmadillo}. It performs a similar calculation as
#'   the standard \emph{R} function \code{filter(x=vec_tor, filter=weight_s, 
#'   method="convolution", sides=1)}, but it's over \emph{6} times faster, and it 
#'   doesn't produce any \emph{NA} values.
#'   
#' @examples
#' \dontrun{
#' # First example
#' # create vector from historical prices
#' vec_tor <- as.numeric(rutils::etf_env$VTI[, 6])
#' # create simple weights
#' weight_s <- c(1, rep(0, 10))
#' # calculate rolling weighted sum
#' weight_ed <- HighFreq::roll_wsum(vec_tor=vec_tor, weight_s=rev(weight_s))
#' # compare with original
#' all.equal(vec_tor, as.numeric(weight_ed))
#' # Second example
#' # create exponentially decaying weights
#' weight_s <- exp(-0.2*1:11)
#' weight_s <- weight_s/sum(weight_s)
#' # calculate rolling weighted sum
#' weight_ed <- HighFreq::roll_wsum(vec_tor=vec_tor, weight_s=rev(weight_s))
#' # calculate rolling weighted sum using filter()
#' filter_ed <- filter(x=vec_tor, filter=weight_s, method="convolution", sides=1)
#' # compare both methods
#' all.equal(filter_ed[-(1:11)], weight_ed[-(1:11)], check.attributes=FALSE)
#' }
#' @export
roll_wsum <- function(vec_tor, weight_s) {
    .Call('_HighFreq_roll_wsum', PACKAGE = 'HighFreq', vec_tor, weight_s)
}

#' Calculate the convolutions of the matrix columns with a vector of weights.
#' 
#' @param mat_rix A numeric \emph{matrix} of data.
#' @param weight_s A column \emph{vector} of weights.
#'
#' @return A numeric \emph{matrix} with the same dimensions as the input
#'   argument \code{mat_rix}.
#'
#' @details The function \code{roll_conv()} calculates the convolutions of the
#'   matrix columns with a vector of weights.  It rolls over the matrix rows
#'   and multiplies the past column values with the weights. It uses the
#'   \emph{RcppArmadillo} function \code{arma::conv2()}. It performs a similar
#'   calculation to the standard \emph{R} function \code{filter(x=mat_rix,
#'   filter=weight_s, method="convolution", sides=1)}, but it's over \emph{6}
#'   times faster, and it doesn't produce any leading \emph{NA} values.
#'   
#' @examples
#' \dontrun{
#' # First example
#' # create matrix from historical prices
#' mat_rix <- na.omit(rutils::etf_env$re_turns[, 1:2])
#' # create simple weights
#' weight_s <- matrix(c(1, rep(0, 10)), nc=1)
#' # calculate rolling weighted sum
#' weight_ed <- HighFreq::roll_conv(mat_rix=mat_rix, weight_s=weight_s)
#' # compare with original
#' all.equal(coredata(mat_rix), weight_ed, check.attributes=FALSE)
#' # Second example
#' # create exponentially decaying weights
#' weight_s <- exp(-0.2*1:11)
#' weight_s <- matrix(weight_s/sum(weight_s), nc=1)
#' # calculate rolling weighted sum
#' weight_ed <- HighFreq::roll_conv(mat_rix=mat_rix, weight_s=weight_s)
#' # calculate rolling weighted sum using filter()
#' filter_ed <- filter(x=mat_rix, filter=weight_s, method="convolution", sides=1)
#' # compare both methods
#' all.equal(filter_ed[-(1:11), ], weight_ed[-(1:11), ], check.attributes=FALSE)
#' }
#' @export
roll_conv <- function(mat_rix, weight_s) {
    .Call('_HighFreq_roll_conv', PACKAGE = 'HighFreq', mat_rix, weight_s)
}

#' Calculate a time series of variance estimates over a rolling look-back
#' interval for an \emph{OHLC} time series of prices, using different range
#' estimators for variance.
#' 
#' Currently only works for vectors
#'
#' @param oh_lc An \emph{OHLC} time series of prices in \emph{xts} format.
#' @param calc_method \emph{character} string representing method for estimating
#'   variance.  The methods include:
#'   \itemize{
#'     \item "close" close to close,
#'     \item "garman_klass" Garman-Klass,
#'     \item "garman_klass_yz" Garman-Klass with account for close-to-open price jumps,
#'     \item "rogers_satchell" Rogers-Satchell,
#'     \item "yang_zhang" Yang-Zhang,
#'    }
#'    (The default is \code{"yang_zhang"})
#' @param look_back The size of the look-back interval, equal to the number of rows
#'   of data used for calculating the variance.
#' @param sca_le \emph{Boolean} argument: should the returns be divided by the
#'   number of seconds in each period? (The default is \code{TRUE})
#'
#' @return An \emph{xts} time series with a single column and the same number of
#'   rows as the argument \code{oh_lc}.
#'
#' @details The function \code{roll_var()} calculates a time series of rolling 
#'   variance estimates of percentage returns, from over a
#'   \emph{vector} of returns, using several different variance estimation
#'   methods based on the range of \emph{OHLC} prices.
#'
#'   If \code{sca_le} is \code{TRUE} (the default), then the variance is divided
#'   by the squared differences of the time index (which scales the variance to
#'   units of variance per second squared.) This is useful for example, when
#'   calculating intra-day variance from minutely bar data, because dividing
#'   returns by the number of seconds decreases the effect of overnight price
#'   jumps.
#'
#'   If \code{sca_le} is \code{TRUE} (the default), then the variance is
#'   expressed in the scale of the time index of the \emph{OHLC} time series.
#'   For example, if the time index is in seconds, then the variance is given in
#'   units of variance per second squared.  If the time index is in days, then
#'   the variance is equal to the variance per day squared.
#'
#'   The time index of the \code{oh_lc} time series is assumed to be in
#'   \emph{POSIXct} format, so that its internal value is equal to the number of
#'   seconds that have elapsed since the \emph{epoch}.
#'
#'   The methods \code{"close"}, \code{"garman_klass_yz"}, and
#'   \code{"yang_zhang"} do account for close-to-open price jumps, while the
#'   methods \code{"garman_klass"} and \code{"rogers_satchell"} do not account
#'   for close-to-open price jumps.
#'
#'   The default method is \code{"yang_zhang"}, which theoretically has the
#'   lowest standard error among unbiased estimators.
#'
#'   The function \code{roll_var()} performs the same calculations as the
#'   function \code{volatility()} from package
#'   \href{https://cran.r-project.org/web/packages/TTR/index.html}{TTR}, but
#'   it's a little faster because it performs less data validation.
#'
#' @examples
#' \dontrun{
#' # create minutely OHLC time series of random prices
#' oh_lc <- HighFreq::random_ohlc()
#' # calculate variance estimates for oh_lc over a 21 period interval
#' var_rolling <- HighFreq::roll_var(oh_lc, look_back=21)
#' # calculate variance estimates for SPY
#' var_rolling <- HighFreq::roll_var(HighFreq::SPY, calc_method="yang_zhang")
#' # calculate SPY variance without accounting for overnight jumps
#' var_rolling <- HighFreq::roll_var(HighFreq::SPY, calc_method="rogers_satchell")
#' }
#' @export
roll_var <- function(vec_tor, look_back) {
    .Call('_HighFreq_roll_var', PACKAGE = 'HighFreq', vec_tor, look_back)
}

#' Perform a rolling scaling (standardization) of the columns of a
#' \emph{matrix} of data using \emph{RcppArmadillo}.
#' 
#' @param mat_rix A numeric \emph{matrix} of data.
#' @param look_back The length of the look-back interval, equal to the number 
#'   of rows of data used in the scaling.
#' @param use_median A \emph{Boolean} argument: if \code{TRUE} then the 
#'   centrality (central tendency) is calculated as the \emph{median} and the 
#'   dispersion is calculated as the \emph{median absolute deviation}
#'   (\emph{MAD}).
#'   If \code{use_median} is \code{FALSE} then the centrality is calculated as 
#'   the \emph{mean} and the dispersion is calculated as the \emph{standard
#'   deviation}. (The default is \code{FALSE})
#'
#' @return A numeric \emph{matrix} with the same dimensions as the input
#'   argument \code{mat_rix}.
#'
#' @details The function \code{roll_scale()} performs a rolling scaling
#'   (standardization) of the columns of the \code{mat_rix} argument using
#'   \emph{RcppArmadillo}.
#'   The function \code{roll_scale()} performs a loop over the rows of 
#'   \code{mat_rix}, then subsets a number of previous (past) rows equal to 
#'   \code{look_back}, and scales the subset matrix.  It assigns the last row
#'   of the scaled subset matrix to the return matrix.
#'   
#'   If the argument \code{use_median} is \code{FALSE} (the default), then it
#'   performs a similar calculation as the function \code{roll::roll_scale()}.
#'   If the argument \code{use_median} is \code{TRUE}, then it calculates the
#'   centrality as the \emph{median} and the dispersion as the \emph{median
#'   absolute deviation} (\emph{MAD}).
#'   
#' @examples
#' \dontrun{
#' mat_rix <- matrix(rnorm(20000), nc=2)
#' look_back <- 11
#' rolled_scaled <- roll::roll_scale(data=mat_rix, width=look_back, min_obs=1)
#' rolled_scaled2 <- roll_scale(mat_rix=mat_rix, look_back=look_back, use_median=FALSE)
#' all.equal(rolled_scaled[-1, ], rolled_scaled2[-1, ])
#' }
#' @export
roll_scale <- function(mat_rix, look_back, use_median = FALSE) {
    .Call('_HighFreq_roll_scale', PACKAGE = 'HighFreq', mat_rix, look_back, use_median)
}

#' Perform rolling regressions over the rows of the design matrix, and
#' calculate a \emph{vector} of z-scores of the residuals.
#' 
#' @param res_ponse A numeric \emph{vector} of response data.
#' @param de_sign A numeric \emph{matrix} of design (predictor i.e.
#'   explanatory) data.
#' @param look_back The length of the look-back interval, equal to the number 
#'   of elements of data used for calculating the regressions.
#'
#' @return A numeric \emph{vector} of the same length as the number of rows of
#'   \code{de_sign}.
#'
#' @details The function \code{roll_zscores()} performs rolling regressions 
#'   along the rows of the design matrix \code{de_sign}, using the function
#'   \code{calc_lm()}. 
#'   
#'   The function \code{roll_zscores()} performs a loop over the rows of 
#'   \code{de_sign}, and it subsets \code{de_sign} and \code{res_ponse} over a 
#'   number of previous (past) rows equal to \code{look_back}.  It performs a 
#'   regression on the subset data, and calculates the \emph{z-score} of the 
#'   last residual value for each regression. It returns a numeric
#'   \emph{vector} of the \emph{z-scores}.
#'   
#' @examples
#' \dontrun{
#' # calculate Z-scores from rolling time series regression using RcppArmadillo
#' look_back <- 11
#' clo_se <- as.numeric(Cl(rutils::etf_env$VTI))
#' date_s <- xts::.index(rutils::etf_env$VTI)
#' z_scores <- HighFreq::roll_zscores(res_ponse=clo_se, 
#'  de_sign=matrix(as.numeric(date_s), nc=1), 
#'  look_back=look_back)
#' # Define design matrix with explanatory variables
#' len_gth <- 100; n_var <- 5
#' de_sign <- matrix(rnorm(n_var*len_gth), nc=n_var)
#' # response equals linear form plus error terms
#' weight_s <- rnorm(n_var)
#' res_ponse <- -3 + de_sign %*% weight_s + rnorm(len_gth, sd=0.5)
#' # calculate Z-scores from rolling multivariate regression using RcppArmadillo
#' look_back <- 11
#' z_scores <- HighFreq::roll_zscores(res_ponse=res_ponse, de_sign=de_sign, look_back=look_back)
#' # calculate z-scores in R from rolling multivariate regression using lm()
#' z_scores_r <- sapply(1:NROW(de_sign), function(ro_w) {
#'   if (ro_w==1) return(0)
#'   start_point <- max(1, ro_w-look_back+1)
#'   sub_response <- res_ponse[start_point:ro_w]
#'   sub_design <- de_sign[start_point:ro_w, ]
#'   reg_model <- lm(sub_response ~ sub_design)
#'   resid_uals <- reg_model$residuals
#'   resid_uals[NROW(resid_uals)]/sd(resid_uals)
#' })  # end sapply
#' # compare the outputs of both functions
#' all.equal(unname(z_scores[-(1:look_back)]), 
#'   unname(z_scores_r[-(1:look_back)]))
#' }
#' @export
roll_zscores <- function(res_ponse, de_sign, look_back) {
    .Call('_HighFreq_roll_zscores', PACKAGE = 'HighFreq', res_ponse, de_sign, look_back)
}

#' Simulate a \emph{GARCH} process using \emph{Rcpp}.
#' 
#' @param om_ega Parameter proportional to the long-term average level of variance.
#' @param al_pha The weight associated with recent realized variance updates.
#' @param be_ta The weight associated with the past variance estimates.
#' @param in_nov A numeric \emph{vector} of innovations (random numbers).
#' 
#' @return A numeric \emph{matrix} with two columns: the simulated returns and
#'   variance, and with the same number of rows as the length of the argument 
#'   \code{in_nov}.
#'
#' @details The function \code{sim_garch()} simulates a \emph{GARCH} process
#'   using \emph{Rcpp}.
#'
#' @examples
#' \dontrun{
#' # Define the GARCH model parameters
#' om_ega <- 0.01
#' al_pha <- 0.5
#' be_ta <- 0.2
#' # Simulate the GARCH process using Rcpp
#' garch_rcpp <- sim_garch(om_ega=om_ega, al_pha=al_pha, be_ta=be_ta, in_nov=rnorm(10000))
#' }
#' @export
sim_garch <- function(om_ega, al_pha, be_ta, in_nov) {
    .Call('_HighFreq_sim_garch', PACKAGE = 'HighFreq', om_ega, al_pha, be_ta, in_nov)
}

#' Simulate an \emph{Ornstein-Uhlenbeck} process using \emph{Rcpp}.
#' 
#' @param eq_price The equilibrium price. 
#' @param vol_at The volatility of returns.
#' @param the_ta The strength of mean reversion.
#' @param in_nov A numeric \emph{vector} of innovations (random numbers).
#' 
#' @return A numeric \emph{vector} representing the time series of prices, with
#'   the same length as the argument \code{in_nov}.
#'
#' @details The function \code{sim_ou()} simulates an \emph{Ornstein-Uhlenbeck}
#'   process using \emph{Rcpp}, and returns a \emph{vector} representing the 
#'   time series of prices.
#'
#' @examples
#' \dontrun{
#' # Define the Ornstein-Uhlenbeck model parameters
#' eq_price <- 5.0
#' vol_at <- 0.01
#' the_ta <- 0.01
#' # Simulate Ornstein-Uhlenbeck process using Rcpp
#' price_s <- HighFreq::sim_ou_rcpp(eq_price=eq_price, vol_at=vol_at, the_ta=the_ta, in_nov=rnorm(1000))
#' }
#' @export
sim_ou <- function(eq_price, vol_at, the_ta, in_nov) {
    .Call('_HighFreq_sim_ou', PACKAGE = 'HighFreq', eq_price, vol_at, the_ta, in_nov)
}

#' Recursively filter a vector of innovations through a vector of \emph{ARIMA} 
#' coefficients.
#' 
#' @param in_nov A numeric \emph{vector} of innovations (random numbers).
#' @param co_eff A numeric \emph{vector} of \emph{ARIMA} coefficients.
#'
#' @return A numeric \emph{vector} of the same length as the argument
#'   \code{in_nov}.
#'
#' @details The function \code{sim_arima()} recursively filters a vector of
#'   innovations through a vector of \emph{ARIMA} coefficients, using 
#'   \emph{RcppArmadillo}.
#'   It performs the same calculation as the standard \emph{R} function 
#'   \code{filter(x=in_nov, filter=co_eff, method="recursive")}, but it's over
#'   \emph{6} times faster.
#'   
#' @examples
#' \dontrun{
#' # create vector of innovations
#' in_nov <- rnorm(100)
#' # create ARIMA coefficients
#' co_eff <- c(-0.8, 0.2)
#' # calculate recursive filter using filter()
#' filter_ed <- filter(in_nov, filter=co_eff, method="recursive")
#' # calculate recursive filter using RcppArmadillo
#' ari_ma <- HighFreq::sim_arima(in_nov, rev(co_eff))
#' # compare the two methods
#' all.equal(as.numeric(ari_ma), as.numeric(filter_ed))
#' }
#' @export
sim_arima <- function(in_nov, co_eff) {
    .Call('_HighFreq_sim_arima', PACKAGE = 'HighFreq', in_nov, co_eff)
}

#' Calculate the optimal portfolio weights for different objective functions.
#' 
#' @param re_turns A numeric \emph{matrix} of excess returns data (the returns
#'   in excess of the risk-free rate).
#' @param typ_e A \emph{string} specifying the objective for calculating the
#'   weights (see Details).
#' @param max_eigen An \emph{integer} equal to the number of eigenvectors used
#'   for calculating the regularized inverse of the covariance matrix (the
#'   default is the number of columns of \code{re_turns}).
#' @param al_pha The shrinkage intensity (the default is \code{0}).
#' @param scal_e A \emph{Boolean} specifying whether the weights should be
#'   scaled (the default is \code{TRUE}).
#'
#' @return A numeric \emph{vector} of the same length as the number of columns
#'   of \code{re_turns}.
#'
#' @details The function \code{calc_weights()} calculates the optimal portfolio
#'   weights for different objective functions, using \emph{RcppArmadillo}.
#' 
#'   If \code{typ_e == "max_sharpe"} (the default) then \code{calc_weights()}
#'   calculates the weights of the maximum Sharpe portfolio, by multiplying the
#'   inverse of the covariance matrix times the mean column returns.
#'   
#'   If \code{typ_e == "min_var"} then it calculates the weights of the minimum
#'   variance portfolio under linear constraints.
#'   
#'   If \code{typ_e == "min_varpca"} then it calculates the weights of the
#'   minimum variance portfolio under quadratic constraints (which is the
#'   highest order principal component).
#' 
#'   If \code{typ_e == "rank"} then it calculates the weights as the ranks
#'   (order index) of the trailing Sharpe ratios of the portfolio assets.
#' 
#'   If \code{scal_e == TRUE} (the default) then \code{calc_weights()} scales
#'   the weights so that the resulting portfolio has the same volatility as the
#'   equally weighted portfolio.
#'   
#'   \code{calc_weights()} applies dimensional regularization to calculate the
#'   inverse of the covariance matrix of returns from its eigen decomposition,
#'   using the function \code{arma::eig_sym()}.
#'   
#'   In addition, it applies shrinkage to the vector of mean column returns, by
#'   shrinking it to its common mean value.
#'   The shrinkage intensity \code{al_pha} determines the amount of shrinkage 
#'   that is applied, with \code{al_pha = 0} representing no shrinkage (with 
#'   the estimator of mean returns equal to the means of the columns of 
#'   \code{re_turns}), and \code{al_pha = 1} representing complete shrinkage 
#'   (with the estimator of mean returns equal to the single mean of all the
#'   columns of \code{re_turns})
#' 
#' @examples
#' \dontrun{
#' # Calculate covariance matrix of ETF returns
#' re_turns <- na.omit(rutils::etf_env$re_turns[, 1:16])
#' ei_gen <- eigen(cov(re_turns))
#' # Calculate regularized inverse of covariance matrix
#' max_eigen <- 3
#' eigen_vec <- ei_gen$vectors[, 1:max_eigen]
#' eigen_val <- ei_gen$values[1:max_eigen]
#' in_verse <- eigen_vec %*% (t(eigen_vec) / eigen_val)
#' # Define shrinkage intensity and apply shrinkage to the mean returns
#' al_pha <- 0.5
#' col_means <- colMeans(re_turns)
#' col_means <- ((1-al_pha)*col_means + al_pha*mean(col_means))
#' # Calculate weights using R
#' weight_s <- in_verse %*% col_means
#' n_col <- NCOL(re_turns)
#' weights_r <- weights_r*sd(re_turns %*% rep(1/n_col, n_col))/sd(re_turns %*% weights_r)
#' # Calculate weights using RcppArmadillo
#' weight_s <- drop(HighFreq::calc_weights(re_turns, max_eigen=max_eigen, al_pha=al_pha))
#' all.equal(weight_s, weights_r)
#' }
#' @export
calc_weights <- function(re_turns, typ_e = "max_sharpe", max_eigen = 1L, al_pha = 0, scal_e = TRUE) {
    .Call('_HighFreq_calc_weights', PACKAGE = 'HighFreq', re_turns, typ_e, max_eigen, al_pha, scal_e)
}

#' Simulate (backtest) a rolling portfolio optimization strategy, using
#' \emph{RcppArmadillo}.
#' 
#' @param ex_cess A numeric \emph{matrix} of excess returns data (the returns
#'   in excess of the risk-free rate).
#' @param re_turns A numeric \emph{matrix} of excess returns data (the returns
#'   in excess of the risk-free rate).
#' @param start_points An integer \emph{vector} of start points.
#' @param end_points An integer \emph{vector} of end points.
#' @param typ_e A \emph{string} specifying the objective for calculating the
#'   weights (see Details).
#' @param max_eigen An \emph{integer} equal to the number of eigenvectors used
#'   for calculating the regularized inverse of the covariance matrix (the
#'   default is the number of columns of \code{re_turns}).
#' @param al_pha A numeric shrinkage intensity.  (The default is \code{0})
#' @param scal_e A \emph{Boolean} specifying whether the weights should be
#'   scaled (the default is \code{TRUE}).
#' @param co_eff A numeric multiplier of the weights.  (The default is
#'   \code{1})
#' @param bid_offer A numeric bid-offer spread.  (The default is \code{0})
#'
#' @return A numeric \emph{vector} of strategy returns, with the same length as
#'   the number of rows of \code{re_turns}.
#'
#' @details The function \code{back_test()} performs a backtest simulation of a
#'   rolling portfolio optimization strategy over a \emph{vector} of
#'   \code{end_points}.
#'   
#'   It performs a loop over the \code{end_points}, and subsets the
#'   \emph{matrix} of excess returns \code{ex_cess} along its rows, between the
#'   corresponding end point and the start point. It passes the subset matrix
#'   of excess returns into the function \code{calc_weights()}, which
#'   calculates the optimal portfolio weights. The arguments \code{max_eigen},
#'   \code{al_pha}, \code{typ_e}, and \code{scal_e} are also passed to the
#'   function \code{calc_weights()}.
#'   
#'   The function \code{back_test()} multiplies the weights by the coefficient
#'   \code{co_eff} (with default equal to \code{1}), which allows reverting a
#'   strategy if \code{co_eff = -1}.
#'   
#'   The function \code{back_test()} then multiplies the weights times the
#'   future portfolio returns, to calculate the out-of-sample strategy returns.
#'   
#'   The function \code{back_test()} calculates the transaction costs by
#'   multiplying the bid-offer spread \code{bid_offer} times the absolute
#'   difference between the current weights minus the weights from the previous
#'   period. It then subtracts the transaction costs from the out-of-sample
#'   strategy returns.
#'   
#'   The function \code{back_test()} returns a time series (column vector) of
#'   strategy returns, of the same length as the number of rows of
#'   \code{re_turns}.
#'
#' @examples
#' \dontrun{
#' # Calculate the ETF daily excess returns
#' re_turns <- na.omit(rutils::etf_env$re_turns[, 1:16])
#' # risk_free is the daily risk-free rate
#' risk_free <- 0.03/260
#' ex_cess <- re_turns - risk_free
#' # Define monthly end_points without initial warmpup period
#' end_points <- rutils::calc_endpoints(re_turns, inter_val="months")
#' end_points <- end_points[end_points>50]
#' len_gth <- NROW(end_points)
#' # Define 12-month look_back interval and start_points over sliding window
#' look_back <- 12
#' start_points <- c(rep_len(1, look_back-1), end_points[1:(len_gth-look_back+1)])
#' # Define shrinkage and regularization intensities
#' al_pha <- 0.5
#' max_eigen <- 3
#' # Simulate a monthly rolling portfolio optimization strategy
#' pnl_s <- HighFreq::back_test(ex_cess, re_turns, 
#'                             start_points-1, end_points-1, 
#'                             max_eigen, al_pha)
#' pnl_s <- xts(pnl_s, index(re_turns))
#' colnames(pnl_s) <- "strat_rets"
#' # Plot dygraph of strategy
#' dygraphs::dygraph(cumsum(pnl_s), 
#'   main="Cumulative Returns of Max Sharpe Portfolio Strategy")
#' }
#' @export
back_test <- function(ex_cess, re_turns, start_points, end_points, typ_e = "max_sharpe", max_eigen = 1L, al_pha = 0, scal_e = TRUE, co_eff = 1.0, bid_offer = 0.0) {
    .Call('_HighFreq_back_test', PACKAGE = 'HighFreq', ex_cess, re_turns, start_points, end_points, typ_e, max_eigen, al_pha, scal_e, co_eff, bid_offer)
}

